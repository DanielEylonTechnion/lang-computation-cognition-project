{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Intro\n",
        "\n",
        "The following code provides you with surprisal values for the same data you used in Homework 2.  The surprisal values are based on the output of an RNN model.\n",
        "\n",
        "We leave it up to you to perform the alignment between the model output and the reading times!\n",
        "\n",
        "Feel free to skim through the code, it might be helpful for the semi-structured and open-ended tasks!"
      ],
      "metadata": {
        "id": "eQ6n3Htlt3y9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln_K1X0KRr8c"
      },
      "source": [
        "## Set up dependencies and data\n",
        "\n",
        "Run the cell below to fetch the recurrent neural network codebase we've designed for this assignment. \n",
        "\n",
        "It will leave the files in a directory called `rnn` (check in the sidebar after you've run the command)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "git clone https://github.com/omershubi/neural-complexity.git rnn\n",
        "mkdir -p rnn/data/ptb"
      ],
      "metadata": {
        "id": "pbRq5Ds9jMeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8krcYzgRvmb"
      },
      "source": [
        "## Mount Google Drive\n",
        "\n",
        "We'll save your corpus data, model checkpoints, and output to Google Drive for safekeeping. \n",
        "\n",
        "Follow the instructions in the output of this command to link your Drive account."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFlrqEPhxvIT"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2Q3yKI4EO-S"
      },
      "source": [
        "GDRIVE_DIR = \"/content/gdrive/My Drive/096222_project_surprisals_and_rts\"\n",
        "!mkdir -p \"$GDRIVE_DIR\"\n",
        "!mkdir -p \"$GDRIVE_DIR/corpus_data\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFVqK01ETKOQ"
      },
      "source": [
        "## Upload corpus files for LM training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPSSxMinJIO4"
      },
      "source": [
        "Here we upload the Penn Treebank corpus files  to the `rnn/data/ptb` folder.\n",
        "Your files should be named:\n",
        "\n",
        "- `ptb_tok_train.txt`\n",
        "- `ptb_tok_dev.txt`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -qO rnn/data/ptb/ptb_tok_train.txt https://gist.githubusercontent.com/omershubi/cdd4231472d6188f03ab21e2b2729fee/raw/e1b4c764561fd038470830534baaa220b0eb4c6d/ptb_tok_train.txt\n",
        "!wget -qO rnn/data/ptb/ptb_tok_dev.txt https://gist.githubusercontent.com/omershubi/31eff71b74dfb8cfe93d1a9acf8ab523/raw/094d3094b06beb92cd7fd0496710cf43273f8c64/ptb_tok_dev.txt\n",
        "!cp rnn/data/ptb/* \"$GDRIVE_DIR/corpus_data/\""
      ],
      "metadata": {
        "id": "Wa4hcnT4mka0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJl-6DkXR3hE"
      },
      "source": [
        "# Recurrent neural network modeling\n",
        "\n",
        "## Train model\n",
        "\n",
        "Now we're ready to begin training the model. The below command will launch the neural network optimization procedure, and progressively save checkpoints to the Google Drive you've just mounted. The neural network train for 40 epochs (1 epoch = 1 loop through the training dataset), which should take approximately **2 hours**.\n",
        "\n",
        "Checkpoints will be saved each time the validation loss improves, and named `ptb_model.pt`. Watch the output of the command below to see how the model's train and validation perplexity improve over time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wm9H0xz8pbE4"
      },
      "source": [
        "!cd rnn && python main.py --cuda --model_file \"$GDRIVE_DIR/ptb_model.pt\" \\\n",
        "    --epochs 40 \\\n",
        "    --vocab_file \"$GDRIVE_DIR/ptb_vocab.txt\" \\\n",
        "    --tied --data_dir \"$GDRIVE_DIR/corpus_data\" --trainfname ptb_tok_train.txt --validfname ptb_tok_dev.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg-ePSJyTSso"
      },
      "source": [
        "## Compute surprisals\n",
        "\n",
        "Now we'll compute model surprisals for the text data in `brown.txt` (already downloaded for you). This will output a file called `rnn_surprisals.tsv` to your Google Drive, which you'll use in the next steps of the analysis.\n",
        "\n",
        "We'll load the checkpoint and use it to extract per-token surprisal estimates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7-kbO7aeQxt"
      },
      "source": [
        "checkpoint_to_use = f\"{GDRIVE_DIR}/ptb_model.pt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_lmhZ1AM53E"
      },
      "source": [
        "!cd rnn && python main.py --cuda --model_file \"$checkpoint_to_use\" \\\n",
        "    --vocab_file \"$GDRIVE_DIR/ptb_vocab.txt\" --data_dir './data' \\\n",
        "    --testfname 'brown.txt' --test --words --nopp > \"$GDRIVE_DIR/rnn_surprisals.tsv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZQgvEMNrtyt"
      },
      "source": [
        "Cool, let's check that the surprisal output looks right. (Compare with the ngram model surprisals from before.)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "rnn_surprisals = pd.read_csv(f'{GDRIVE_DIR}/rnn_surprisals.tsv',sep=' ')\n",
        "rnn_surprisals"
      ],
      "metadata": {
        "id": "rBNCfCMyerkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PknaMlKW07p"
      },
      "source": [
        "## Harmonize RNN surprisal and RT data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and align the reading times with the n-gram model surprisals.\n",
        "You may load the harmonized csv from Homework 2, or align it again.\n",
        "\n",
        "Finallyy, align the RNN model-derived surprisal values with human reading times. The harmonize function you wrote in Homework 2 may be helpful here too.\n",
        "\n",
        "Note, modifications may be required as the tokenization is not necessarily the same as the n-gram model. "
      ],
      "metadata": {
        "id": "psUdCeJysjGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "EUCKCUuTsg7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next Steps\n",
        "\n",
        "For the next steps, follow the PDF project instructions.\n",
        "\n",
        "Good luck!"
      ],
      "metadata": {
        "id": "DgFF4lLQtQzw"
      }
    }
  ]
}